<!DOCTYPE html>
<html>
  <head>
    <title>The Impact of Attention</title>
    <link rel="stylesheet" type="text/css" href="./index.css">
    <script src="https://d3js.org/d3.v5.js"></script>
  </head>
  <body>
    <header class="stone-background">
      <div class="vertical-padding">
	<h1 style="">The Impact of Attention</h1>
	<p>A demonstration of how attention mechanisms can improve the accuracy of RNN architectures.</p>
      </div>
    </header>
    <section id="introduction">
      <div class="horizontal-padding vertical-padding">
	<h3>The Leaky Neural Network Abstraction Fallacy</h3>
	<p>A colleague from one of my reading groups asked, "Since an LSTM has memory, can't it simply remember which word vectors are important and encode important parts of vectors worth paying attention to in its hidden state? It doesn't seem practical for us to be learning about attention mechanisms if an LSTM could simply learn the correct function to weed out unimportant information. We'd be wasting time learning a technique that doesn't add anything."</p>
	<p>As <a target="_blank" href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#:~:text=%3E%20The%20problem%20with%20Backpropagation%20is,them%20work%E2%80%9D%20on%20your%20data.">Andrej Karpathy</a> states, "Backpropagation is a <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" target="_blank">leaky abstraction</a>... If you try to ignore how it works under the hood because 'TensorFlow automagically makes my networks learn', you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks."</p>
	<p>My colleague makes the exact mistake that <a target="_blank" href="https://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a> points to by believing that an LSTM can magically learn the exact parameters necessary to get the best results possible.</p>
	<p>For a theoretical explanation of why my colleague's assumptions are flawed or how attention mechanism work, there are many resources elsewhere that cover this (e.g. <a target="_blank" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">here</a>), so I won't repeat what's already been said.</p>
	<p>Instead, I'll provide evidence against the claim that an LSTM can implicitly learn how to perform attention (and thus attention mechanisms don't contribute anything to an LSTM) by showing empirical evidence that an LSTM with an explicit attention mechanism can outperform a vanilla LSTM in terms of training time, loss minimization, and loss minimization per parameter (i.e. the LSTM models with attention can achieve smaller loss with fewer parameters than vanilla LSTMs).</p>
      </div>
    </section>
    <section id="experiment-overview" class="stone-background">
      <div class="horizontal-padding vertical-padding">
	<h3>Experiment Overview</h3>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
      </div>
    </section>
    <section id="experiment-results">
      <div class="horizontal-padding vertical-padding">
	<h3>Experiment Results</h3>
	<div id="bar-chart" class="svg-container-center">
	  <svg id="bar-chart-svg"></svg>
	  <script src="bar_chart.js"></script>
	</div>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<div id="scatter-plot" class="svg-container-center">
	  <svg id="scatter-plot-svg"></svg>
	  <script src="scatter_plot.js"></script>
	</div>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<div id="line-graph" class="svg-container-center">
	  <svg id="line-graph-svg"></svg>
	  <script src="line_graph.js"></script>
	</div>
      </div>
    </section>
    <section id="conclusion" class="stone-background">
      <div class="horizontal-padding vertical-padding">
	<h3>Concluding Remarks</h3>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
	<p>@todo fill this in</p>
      </div>
      <table style="table-layout: fixed; width: 100%; padding-top: 40px; padding-bottom: 40px;">
      	<tr>
      	  <td style="width:10%;"></td>
      	  <td style="width:30%;">
      	    <card class="stone-background">
      	      <a target="_blank" href="https://github.com/paul-tqh-nguyen">
      		<div class="card-text">
      		  <p>Interested in my work?</p>
      		  <p><b>See my projects on GitHub.</b></p>
      		</div>
      	      </a>
      	    </card>
      	  </td>
      	  <td style="width:20%;"></td>
      	  <td style="width:30%;">
      	    <card class="stone-background">
      	      <a target="_blank" href="https://paul-tqh-nguyen.github.io/about/">
      		<div class="card-text">
      		  <p>Want to learn more about me?</p>
      		  <p><b>Visit my website.</b></p>
      		</div>
      	      </a>
      	    </card>
      	  </td>
      	  <td style="width:10%;"></td>
      	</tr>
      </table>
    </section>
  </body>
</html>
