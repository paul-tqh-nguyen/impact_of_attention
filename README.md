# The Impact Of Attention

This repository contains functionality and documentation of an experiment intended to give evidence that attention mechanisms can improve the performance of LSTM-based deep-learning models.

The documentation of the experiment can be found at https://paul-tqh-nguyen.github.io/impact_of_attention/

We compared vanilla LSTMs to LSTMs with attention (we focused on [Zhouhan Lin's self-attention mechanism](https://arxiv.org/abs/1703.03130)) on the [IMDB binary sentiment classification dataset](http://ai.stanford.edu/~amaas/data/sentiment/).

We implemented our models using Pytorch and TorchText.

Visualizations were made using [D3.js](https://d3js.org/).